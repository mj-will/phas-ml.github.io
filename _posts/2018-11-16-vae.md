---
layout: post
title:  "A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music"
date:   2018-11-16
categories: paper
speaker: Anna Fumagalli
---

At this reading group we discussed a brief overview of Autoencoders and 
took a look at MusicVAE - a Variational Autoencoder model for blending and exploring musical scores.

# Abstract

> The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online.

Here is the link to the [paper]. 
There is a [blog post] that discusses the paper implementation and graphical interface for the implementation.
Moreover, here is a nice general overview of VAE [useful link].

[paper]: https://arxiv.org/abs/1803.05428
[useful link]: https://www.jeremyjordan.me/variational-autoencoders/
[blog post]: https://magenta.tensorflow.org/music-vae
