{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWT17pCc2OiVractmZz+ID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mj-will/phas-ml.github.io/blob/master/notebooks/2020_10_05_basic_nn_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1aU9Fndf9g",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4exIkEwJQkB"
      },
      "source": [
        "# Introduction to Neural Networks for Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulTgThFTZQN"
      },
      "source": [
        "This post covers some of the basic concepts that are necessary to understand neural networks and implement them for basic supervised classification and regression task. It's copied largely word for word from this post I wrote for our last tutorial on the same topic: https://mj-will.github.io/intro2ml/topics/t0_classification_regression.html\n",
        "\n",
        "\n",
        "## What is a neural network?\n",
        "\n",
        "A neural network is a function that learns a mapping from an input $x$ and produces an output $y$:\n",
        "\n",
        "$$y = f_{\\text{NN}}(x)$$\n",
        "\n",
        "The core building block that allows a neural network to learn are **neurons**, they have parameters that can be updated which allowing for them to learn a mapping. For a input vector $\\mathbf{x}$ a neuron is defined as:\n",
        "\n",
        "$$y = \\sigma (\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
        "\n",
        "where $\\mathbf{w}$ is a vector of trainable parameters known as the **weights**, $b$ a scalar value that is also trainable and known as the **bias** and $\\sigma$ a chosen function, known as an **activation function**, that is normally non-linear and should be differentiable. Neurons are often depicted as nodes with inputs and outputs:\n",
        "\n",
        "The non-linearity of activation functions is what allows an neural network to learn complex non-linear mappings. There are numerous different functions that are used but perhaps the most common is the rectified linear unit or ReLU for short. It is defined as:\n",
        "\n",
        "$$\\sigma(x)= \\begin{cases} x \\quad & \\text{if} \\ x > 0 \\\\ 0 & \\text{elsewhere} \\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9e_j040VM2I",
        "cellView": "form"
      },
      "source": [
        "#@title Relu\n",
        "def ReLU(x):\n",
        "    return x.clamp(min=0)\n",
        "x = torch.linspace(-5, 5, 1000)\n",
        "y = ReLU(x)\n",
        "plt.plot(x,y )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG80TGzEVIj6"
      },
      "source": [
        "\n",
        "\n",
        "Other common activations functions include TanH, Exponential Linear Unit (ELU) and Leaky ReLU. See [here](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html) for more information.\n",
        "\n",
        "\n",
        "\n",
        "### From a single neuron to a network\n",
        "\n",
        "A single neural network contains multiple neurons and they are arranged in layers.\n",
        "\n",
        "\n",
        "Each layer is a vector function (it returns a vector) that has a weights matrix $\\mathbf{W}_{\\text{l}}$ and bias vector $\\mathbf{b}_{\\text{l}}$:\n",
        "\n",
        "$$\\mathbf{f}_{\\text{l}}(\\mathbf{x}) = \\mathbf{\\sigma}_{\\text{l}} (\\mathbf{W}_{\\text{l}} \\cdot \\mathbf{x} + \\mathbf{b}_{\\text{l}})$$\n",
        "\n",
        "A neural network therefore behaves like a set of nested functions, for three layers this would be:\n",
        "\n",
        "$$y = f_{\\text{NN}} (\\mathbf{x}) = \\mathbf{f}_{3}(\\mathbf{f}_{2}(\\mathbf{f}_{1}(\\mathbf{x}))$$\n",
        "\n",
        "\n",
        "The form of the final layer depends on the task at hand and we will focus on this later on. Now that we have an rough description of a neural network we have to understand how a neural network is trained, that is, how does it learn?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhRrgcKpbkxq"
      },
      "source": [
        "\n",
        "\n",
        "## How does a neural network learn?  \n",
        "\n",
        "The answer boils down to a **loss function**, **backpropogation** and **gradient descent**, so what are they?\n",
        "\n",
        "### Loss function\n",
        "\n",
        "In order for a neural network, or indeed almost any machine learning algorithm, to learn it needs a function to describe it's current performance. This function is known as a **loss function** or cost function and, in the case of a supervised task, it describes the difference between the networks output and the ground truth. An example of a loss function is the **mean squared error** (MSE) which, for a vector of predicted values $\\mathbf{\\hat{y}}$ and a vector of true values $\\mathbf{y}$ both of length $N$ is defined as:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} - \\hat{y}_{i})^{2}$$\n",
        "\n",
        "Such as function can then be used to update the trainable parameters of the network with an algorithm such as backpropogation.\n",
        "\n",
        "### Backpropogration and gradient descent\n",
        "\n",
        "**Backpropogation** is an algorithm that calculates the gradient of the loss function with respect to the neural network's weights. It uses the chain rule and proceeds backwards through the network from the output through all of the layers. The gradients can the be used to change the parameters and try to minimise (or maximise) the loss function. \n",
        "\n",
        "This minimisation is then acheived using a **gradient descent algorithm**, such as stochastic graident descent, that explores the parameter space described by the networks weights. This is done in steps where the some data $\\mathbf{x}$ is propogated through the network in a **forward pass** and the output $\\mathbf{y}$, with the change in the loss function, used to update the weights.\n",
        "\n",
        "For a more indepth explanation see [here](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html).\n",
        "\n",
        "Up until this point most of statements about neural networks have been problem agnostic but now we will focus on the specifics of two common types of problems: classification and regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ovb3EHPbyXI"
      },
      "source": [
        "\n",
        "## Differences between classification and regression\n",
        "\n",
        "The main difference between classification and regression problems is what the network is trying to predict. \n",
        "\n",
        "In **classification** the goal is to correctly identify the **class** an input corresponds to, this could be distinguishing between images of cats and dogs or between types of signals. The output is normally a predicted class or a \"probability\" for each of the possible classes (the sum of which is one).\n",
        "\n",
        "For **regression** the goal is to predict a value (or values) given an input, for example predicting housing prices or the frequency of signal. In this case the output is continuous over some range (or potentially unbounded) and the network must output a number (or vector of numbers).\n",
        "\n",
        "So the core difference is the output and this is reflected in the activation function used in the last layer of the network. If the output is different then the same applied the function that quantifies the \"quality\" of the networks output, the loss function. \n",
        "\n",
        "### Classification\n",
        "\n",
        "#### The sigmoid function and binary cross-entropy\n",
        "\n",
        "This is limited to binary classification and outputs a number in the range $[0, 1]$ that can be considered a probability:\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvLn9WxNaOmx",
        "cellView": "form"
      },
      "source": [
        "#@title Sigmoid\n",
        "def sigmoid(x):\n",
        "    return torch.div(1,  1 + torch.exp(-x))\n",
        "\n",
        "x = torch.linspace(-5, 5, 1000)\n",
        "y = sigmoid(x)\n",
        "plt.plot(x,y )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS6e_5M1aMRY"
      },
      "source": [
        "\n",
        "\n",
        "The loss function for this case is **binary cross-entropy** (log-loss) which for $N$ samples is defined as:\n",
        "\n",
        "$$\\text{BCE} = - \\sum_{i=1}^{N} y_{i} \\log_{e}\\left(\\hat{y}_{i}\\right) + (1 - y_{i}) \\log_{e}\\left(1 - \\hat{y}_{i}\\right)$$\n",
        "\n",
        "where $\\hat{y}_{i}$ is the probability for a particular sample $i$ and the $y_{i}$ the true value for the same sample.\n",
        "\n",
        "#### The softmax function and cross-entropy\n",
        "\n",
        "This is the generlised version of the sigmoid for n-class problems. The outputs are again defined on $[0,1]$ and importantly their sum is equal to 1. It is defined as:\n",
        "\n",
        "$$\\sigma(\\mathbf{x})_{i} = \\frac{e^{x_{i}}}{\\sum_{j=1}^{C} e^{x_{j}}} \\ \\text{for} \\ i=1, ..., C \\ \\text{and} \\ \\mathbf{x} \\in \\mathbb{R}^{C}$$\n",
        "\n",
        "where $C$ is the number of classes. The loss function is the generalised version of binary cross-entropy, cross-entropy:\n",
        "\n",
        "$$\\text{CE} = - \\sum_{i=1}^{N} \\sum_{j=i}^{C} y_{ij} \\log_{e}(\\hat{y}_{ij}) $$\n",
        "\n",
        "### Regression\n",
        "\n",
        "#### The indentity\n",
        "\n",
        "In regression problems the output of the neural network generally needs to be continous and unbounded so the activation function is simply the identity:\n",
        "\n",
        "$$\\sigma(x) = x $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPGSVVvca0P_",
        "cellView": "form"
      },
      "source": [
        "#@title Linear\n",
        "def linear(x):\n",
        "    return x\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "y = linear(x)\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-kTtreGax8n"
      },
      "source": [
        "There is no one single loss function to use for regression task but the following are some of the most commonly used.\n",
        "\n",
        "#### Mean squared error\n",
        "\n",
        "Probably the go-to loss function for regression but is sensitive to outliers and will be heavily affected by single vey bad predictions.\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} - \\hat{y}_{i})^{2}$$\n",
        "\n",
        "#### Mean absolute error\n",
        "\n",
        "Very similar to MSE but not as sensitive to outliers.\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_{i} - \\hat{y}_{i}| $$\n",
        "\n",
        "#### Mean squared logarithmic error\n",
        "\n",
        "The mean squared logarithmic error is well suited to problems where the values a large and you are more concerned with relative errors.\n",
        "\n",
        "$$\\text{MSLE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left[\\log_{e}(y_{i}+1) - \\log_{e}(\\hat{y}_{i}+1)\\right]^{2}}$$\n",
        "\n",
        "#### Other loss functions\n",
        "\n",
        "The are numerous other loss functions that can be used in regression such as: root mean squared error, mean squared percentage error, R-squared... Each is best suited to particular use cases but those mentioned before will work in most situations. For more complete list see [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZzuvjoUJcPg"
      },
      "source": [
        "# On to a worked example\n",
        "\n",
        "The following example contains the skeleton what you need to train a neural network. Most of the cells will need you to complete missing variables and define some functions.\n",
        "\n",
        "There are also some additional *futher exercises* which are not necessary but are a good way of becoming more familiar with `torch`.\n",
        "\n",
        "I suggest going through the main sections first and then coming back to the further exercises if you're interested. I have tried to provided a description of what you need to do in each cell but I'm expecting you to have to check the `torch` documentation for lists of functions and modules There's a lot of functionality in `torch`, so the documentation can seem a bit daunting, for the majority of this example you shouldn't need to stray to far from the `nn` module: https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "## Structure\n",
        "\n",
        "This example has three main sections:\n",
        "\n",
        "* Dataset\n",
        "* Model\n",
        "* Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PITPcQVIJvvq"
      },
      "source": [
        "## Dataset - Fashion MNIST\n",
        "\n",
        "Most of us are familiar with MNIST, the dataset of handwritten digits  that is often used as a simple benchmark for machine learning. It's not hard to get close to 100% accuracy. Since the datasets machine learning is being applied to are getting more complicated there has been a movement towards more complex datasets for benchmarking.\n",
        "\n",
        "Fashion MNIST is one such dataset, it contains **ten classes** like the original MNIST:\n",
        "\n",
        "* 0 T-shirt/top\n",
        "* 1 Trouser\n",
        "* 2 Pullover\n",
        "* 3 Dress\n",
        "* 4 Coat\n",
        "* 5 Sandal\n",
        "* 6 Shirt\n",
        "* 7 Sneaker\n",
        "* 8 Bag\n",
        "* 9 Ankle boot\n",
        "\n",
        "The images are same size as the original MNIST (**28 x 28**) and have values between **0 and 255**. So it serves as a drop in replacement for the original MNIST.\n",
        "\n",
        "We're now going to load the data and have a look at one of the images. This part is setup and you don't need to tweak anything, since loading the data can be very specific to your particular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174yIFiOaD6y"
      },
      "source": [
        "FashionMNIST is include in torch's vision library `torchvision`, so we can download and load it from there. This will give us a `Dataset` object which generally make data loading easy. The can also include any pre-processing that\n",
        "you might need to apply to your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZU1g0uEJyU4"
      },
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmBakO6OKDos"
      },
      "source": [
        "\n",
        "#### Tranforms\n",
        "\n",
        "As metioned before, each pixel can take a value between 0 and 255. It's generally recommended to normalise or rescale your data if possible since this usually helps training. In this case we'll rescale to [0, 1] and include this in the set of pre-processing operations that are performed when loading the data.\n",
        "\n",
        "We're also going to apply a transform to convert the data to a `Tensor`. These are very similar to `np.ndarray` but include extra bells and whistles for things like automatic differentiation and performing operations on GPUs. We don't need to worry to much about the details, but your data should always be converted to a `Tensor` before using it to train.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKqCXmnyNDlU"
      },
      "source": [
        "def scale(x):\n",
        "    \"\"\"Rescale data from [0, 255] to [0, 1]\"\"\"\n",
        "    return x / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uACg3Lu3bWVq"
      },
      "source": [
        "data_transforms = transforms.Compose([transforms.ToTensor(), scale])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaNt9W9BbRVe"
      },
      "source": [
        "PyTorch can load the dataset from an existing location or download it with `download=True`. If you were running things on a local machine you might want to have a directory with datasets in to avoid multiple copies.\n",
        "\n",
        "#### Training and testing\n",
        "\n",
        "FashionMNIST contains two sets of data, one for training (60K) and another smaller dataset for testing (10K). We can load them by setting the `train=True` or `train=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PBXduUsJ3CX"
      },
      "source": [
        "# We give a location to download the data too, whether it's train or test and our list of data transforms.\n",
        "train_dataset = FashionMNIST('./dataset/', train=True, download=True, transform=data_transforms)\n",
        "test_dataset = FashionMNIST('./dataset/', train=False, download=False, transform=data_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7E2Fk-9ciE4"
      },
      "source": [
        "We can now have a look at the one of the images. Notice we don't need to convert to a numpy array for plotting, this is all handled by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i4GvtPqKX8g"
      },
      "source": [
        "plt.imshow(train_dataset[0][0][0], cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7AvG_M8LPOL"
      },
      "source": [
        "### Dataloaders\n",
        "\n",
        "We'll need to iterate through the data to train the neural network, whilst you could do this manually a `Dataloader` makes the whole process simpler and it call also do useful things like shuffle data and use mutiple workers to load data in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIMeTfJLLNua"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1000)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPkrJkgGeY57"
      },
      "source": [
        "Half the work when trying to training a machine learning model on a new data is correctly loading and pre-processing the data. The process will vary a lot depending on data formats, ammount of data etc but I'd recommend having a look and other examples from the `torch` tutorials since these highlight some of the different methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xdmRMnMiC2g"
      },
      "source": [
        "## The Model\n",
        "\n",
        "\n",
        "Time to define the model. In `Torch` the model is defined as class that inherits froms `torch.nn.Module` and should have a `forward` method. \n",
        "\n",
        "If we think of a neural network as a function that maps an input $x$ to an output $y$: \n",
        "\n",
        "$$y = f_{\\text{NN}}(x)$$\n",
        "\n",
        "`forward` corresponds to the definition of this function $f_{\\text{NN}}$. \n",
        "\n",
        "The other key components are the weights and biases. Now you could define everything explicitly starting from vectors and matrices or you can use the built-in modules in `PyTorch`.\n",
        "\n",
        "You define the modules in `__init__` after initialising the base class with `super` and any modules defined here will be correctly registered and their parameters will be included in the parameters of the whole model.\n",
        "\n",
        "For example a simple model could look like this:\n",
        "\n",
        "```python\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(256, 64)\n",
        "        self.outpt_layer = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = F.sigmoid(x)\n",
        "        return x\n",
        "```\n",
        "This is a neural network that expects an input vector of length **256 long** (16 x 16) and has **64 neurons** in the first layer. A **ReLU** activation is applied to the output of the first layer and parsed to second layer which takes **64 inputs** and returns **1 output** which then has a **sigmoid** applied to it.\n",
        "\n",
        "There are a lot of different layers available in `torch`: https://pytorch.org/docs/stable/nn.html. \n",
        "\n",
        "**Hint:** remember to think about how many outputs are needed for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9svLdaAi2EA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK70KTUZiFS8"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Intialise the base class\n",
        "        super(Model, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward function\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSOHK-UxZqi1"
      },
      "source": [
        "Then create an instance of `Model` and send it to the GPU (you might need change the runtime type in `Runtime`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqPPUFe2Di4N"
      },
      "source": [
        "model = Model()\n",
        "device = torch.device('cuda')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj6v2tuYohi1"
      },
      "source": [
        "### Further exercise: Defining your own activation function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIJkYzyXmRKX"
      },
      "source": [
        "\n",
        "Try defining your own activation function. The simplesy way to do this is to implement it as a function using `torch`. For example try implementing the Sigmoid-Weighted Linear Unit or SiLU:\n",
        "\n",
        "$$\n",
        "\\text{SiLU}(x) = x \\: \\text{sigmoid}(x)\n",
        "$$\n",
        "\n",
        "and then use it in your model. It might also be helpful to plot a side-by-side comparison with ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3yIfj54pRYM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhEuUyOEiTZN"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "The choice of loss function depends on the problem. In this case we 10 classes, so we should look to use something like categorical cross entropy.\n",
        "\n",
        "**Danger:** Read the inputs to the torch loss functions very carefully, not all them expect the outputs from a softmax\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvUnEUzOiQTa"
      },
      "source": [
        "loss_fn = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHWiRf98ukd"
      },
      "source": [
        "### Further exercise: Defining your own loss function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zH0jfhtmPqh"
      },
      "source": [
        "\n",
        "Assuming you only use functions from `torch` then loss functions can be implemented as a normal python function.\n",
        "\n",
        "Most of the usual loss functions are already implemented in `torch` but it can be usual exercise to implmented something like Mean Squared Error yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7KDrSrb-iy2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_x1ZL1MiFnH"
      },
      "source": [
        "## Optimiser - The tool for training the model\n",
        "\n",
        "The optimiser is the 'machinery' that will actually update the trainable parameters of your model given the gradients for some inputs. Most optimisers use some from of **stochatic gradient decent** and they all one key parameter common, the **learning rate** which determines the size of the 'steps' taken by the optimiser as it updates the weights.\n",
        "\n",
        "Some commonly used optimisers are:\n",
        "\n",
        "* Stochastic gradient descent\n",
        "* Stochastic gradient descent  with momentum\n",
        "* Adam\n",
        "* Adagrad\n",
        "* RMSProp\n",
        "\n",
        "The optimisers available in `torch` are list here: https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "You need to choose an optimiser and create an instance of it which is set to update the parameters in your model. You can also set a learning rate, but the default values should work to begin with.\n",
        "\n",
        "\n",
        "**Hint**: you can retrieve the parmaters of the model using `model.parameters()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM2iOFZRoWT2"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szhF6EUHikJ_"
      },
      "source": [
        "optimiser = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqIl23BiYQ4"
      },
      "source": [
        "## Training\n",
        "\n",
        "We now all the tools to train our model. There's other functionality we could include that would more setup but we don't need this for now.\n",
        "\n",
        "Below is a basic training loop with the steps for updating the parameters of the model already setup. You need to:\n",
        "\n",
        "* Compute the outputs of the model `y_pred` given a batch of data `x`\n",
        "* Compute the loss using `y` (the labels) and `y_pred`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnTN_YOKjZGS"
      },
      "source": [
        "### Further exercise: validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xojsyMAmOKZ"
      },
      "source": [
        "\n",
        "\n",
        "It's often useful to allocate some of the data to use for validation during training and compute a validation loss. This allows you to determine if the model is overfitting (think memorising the training data) rather than learning the general properties.\n",
        "\n",
        "In an ideal case you'd have seperate validationa and test datasets. For ease, in this case we can use the test set.\n",
        "\n",
        "**Task:** compute validation loss using the test data during training.\n",
        "\n",
        "**Hint:** you don't need to compute gradients for the validation data, see how we compute predictions for the test data after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc-RzmsDlNYY"
      },
      "source": [
        "### Further exercise: saving the best model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh60TF9BmMum"
      },
      "source": [
        "\n",
        "If you've computed a validation loss then you can check to see if it starts to increase during training. This is an indication of overfitting. If we're going to use the trained model we want to use the version had the best performance of the validation set, so we should save this model during training.\n",
        "\n",
        "**Task:** save the best model during training\n",
        "\n",
        "**Hint:** You can either save the weights or the whole model. If you were going to save to a file, saving the weights is generally better practice.\n",
        "\n",
        "**Hint:** `deepcopy` can be useful here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh4p80VHmfj6"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gj-1Jbl-10S"
      },
      "source": [
        "n_epochs = 10    # Number of complete passes on the data\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    # set the model to train mode, this is necessary for modules like batchnorm\n",
        "    # and dropout\n",
        "    model.train()\n",
        "    # loop over each batch\n",
        "    for idx, data in enumerate(train_loader):\n",
        "        # in this case `data` is tuple\n",
        "        x = \n",
        "        y = \n",
        "        # reset the gradients for all Variables\n",
        "        optimiser.zero_grad()\n",
        "        # compute the output of the network\n",
        "        y_pred = \n",
        "        # compute the loss\n",
        "        loss = \n",
        "        # compute the gradients\n",
        "        loss.backward()\n",
        "        # update the Variables given the gradient\n",
        "        optimiser.step()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMWVNW2STJ8e"
      },
      "source": [
        "### Understanding the training\n",
        "\n",
        "Plotting the value of the loss function per epoch helps to understand what's going on during the training. As it stands the current training loop\n",
        "does not record this value. It should be realtively easy to do this and print the values at the end of each epoch.\n",
        "\n",
        "**Hint:**\n",
        " you'll need to get the value of the loss, `loss.item()` will be helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5yCFC6xnWSI"
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vI9DyvKFWVs"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Now that the model is training we can evaluate its performance on the test data. This is very similar to training but we don't need to compute any grdients since we won't be updating the weights. You should either get an array of predicted classes for the test data or an array of probabilities for each class for each image.\n",
        "\n",
        "**Hint:**  tensors on can easily be detached from the graph using `x.detach()`, moved to the CPU using `x.cpu()` and converted to numpy using `x.numpy()`. You can even do this: `x.detach().cpu().numpy()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRx291NmFbfG"
      },
      "source": [
        "# Tell the model you're evaluating it\n",
        "# this turns of things like batchnorm\n",
        "model.eval()\n",
        "\n",
        "for idx, data in enumerate(test_loader):\n",
        "    x = \n",
        "    y = \n",
        "    # No need to compute gradients, this is quicker and more memory efficient\n",
        "    with torch.no_grad():\n",
        "        y_pred = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKE35QlnEq6"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "We can now compute a test accuracy. Whilst this is an easy function to write yourself, this is a good opportunity to introduce `sklearn.metrics` which has various metrics predefined such as ROC curves and confusion matrices:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGjvAhDlIfY"
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oXX-pJDn0k0"
      },
      "source": [
        "true_classes = test_dataset.targets.numpy()\n",
        "# For this function you must use the predicted class, not the probability\n",
        "predicted_classes = \n",
        "accuracy = metrics.accuracy_score(true_classes, predicted_classes)\n",
        "print(f'Your final test accuracy is: {accuracy * 100} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujq98cuoCjbC"
      },
      "source": [
        "How did your model do? Here's a link to repo that hosts the data and has some results for different models: https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfn4ajeox--"
      },
      "source": [
        "### Further exercise: confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCnYwAd2o72x"
      },
      "source": [
        "\n",
        "Use `sklearn.metrics` to compute the confusion matrix for the predicted classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3n5ywWSoxPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}